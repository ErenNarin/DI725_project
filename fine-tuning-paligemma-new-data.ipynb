{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install tensorflow overrides ml_collections sentencepiece einops~=0.7 jax ipython pillow scikit-image matplotlib flax kagglehub polars\n",
    "# !pip install -U \"jax[cuda12]\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6041,
     "status": "ok",
     "timestamp": 1745492790880,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "DfxKb3F839Ks",
    "outputId": "6966f47c-5877-4215-d6e4-e45c180ddbf9"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Fetch big_vision repository if python doesn't know about it and install\n",
    "# dependencies needed for this notebook.\n",
    "if not os.path.exists(\"big_vision_repo\"):\n",
    "    !git clone --quiet --branch=main --depth=1 https://github.com/google-research/big_vision big_vision_repo\n",
    "\n",
    "# Append big_vision code to python import path\n",
    "if \"big_vision_repo\" not in sys.path:\n",
    "    sys.path.append(\"big_vision_repo\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15694,
     "status": "ok",
     "timestamp": 1745492819829,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "dTfe2k8J4Bw0",
    "outputId": "bd74dc89-8f1a-4ca4-98f6-78efd73c3535"
   },
   "source": [
    "import base64\n",
    "import functools\n",
    "import html\n",
    "import io\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import sentencepiece\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from PIL import Image\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import model definition from big_vision\n",
    "from big_vision_repo.big_vision.models.proj.paligemma import paligemma\n",
    "from big_vision_repo.big_vision.trainers.proj.paligemma import predict_fns\n",
    "\n",
    "# Import big vision utilities\n",
    "from big_vision_repo.big_vision.datasets.jsonl import DataSource\n",
    "from big_vision_repo.big_vision.utils import tree_map_with_names, reshard, create_learning_rate_schedule, \\\n",
    "    tree_flatten_with_names\n",
    "from big_vision_repo.big_vision.sharding import infer_sharding\n",
    "\n",
    "# Don't let TF use the GPU or TPUs\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "tf.config.set_visible_devices([], \"TPU\")\n",
    "\n",
    "backend = jax.extend.backend.get_backend()\n",
    "print(f\"JAX version:  {jax.__version__}\")\n",
    "print(f\"JAX platform: {backend.platform}\")\n",
    "print(f\"JAX devices:  {jax.device_count()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11302,
     "status": "ok",
     "timestamp": 1745492886506,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "gQNOTfF24AV4",
    "outputId": "dd101cd0-b703-4c50-976b-a73d099a10c4"
   },
   "source": [
    "os.environ[\"KAGGLE_USERNAME\"] = \"erennarin\"  # TODO: remove credentials\n",
    "os.environ[\"KAGGLE_KEY\"] = \"API_KEY\"\n",
    "\n",
    "# Use these for PaliGemma-2 3B 224pxÂ²\n",
    "LLM_VARIANT = \"gemma2_2b\"\n",
    "MODEL_PATH = \"./paligemma2-3b-pt-224.b16.npz\"\n",
    "KAGGLE_HANDLE = \"google/paligemma-2/jax/paligemma2-3b-pt-224\"\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading the checkpoint from Kaggle, this could take a few minutes....\")\n",
    "    MODEL_PATH = kagglehub.model_download(KAGGLE_HANDLE, MODEL_PATH)\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "\n",
    "TOKENIZER_PATH = \"model/paligemma_tokenizer.model\"\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    print(\"Downloading the model tokenizer...\")\n",
    "    !gsutil cp gs://big_vision/paligemma_tokenizer.model {TOKENIZER_PATH}\n",
    "    print(f\"Tokenizer path: {TOKENIZER_PATH}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1aghcULcEdtv"
   },
   "source": [
    "# Define model\n",
    "\n",
    "# IMPORTANT: Gemma-2 has a \"final_logits_softcap\" property. Set it to 0.0\n",
    "# for better transfer results.\n",
    "model_config = ml_collections.FrozenConfigDict({\n",
    "    \"llm\": {\"vocab_size\": 257_152, \"variant\": LLM_VARIANT, \"final_logits_softcap\": 0.0},\n",
    "    \"img\": {\"variant\": \"So400m/14\", \"pool_type\": \"none\", \"scan\": True, \"dtype_mm\": \"float16\"}\n",
    "})\n",
    "model = paligemma.Model(**model_config)\n",
    "tokenizer = sentencepiece.SentencePieceProcessor(TOKENIZER_PATH)\n",
    "\n",
    "# Load params - this can take up to 1 minute in T4 colabs.\n",
    "params = paligemma.load(None, MODEL_PATH, model_config)\n",
    "\n",
    "# Define `decode` function to sample outputs from the model.\n",
    "decode_fn = predict_fns.get_all(model)['decode']\n",
    "decode = functools.partial(decode_fn, devices=jax.devices(), eos_token=tokenizer.eos_id())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16971,
     "status": "ok",
     "timestamp": 1745494959645,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "RWOdf_fw2SAO",
    "outputId": "f5b39426-33d2-409c-ca24-27669b04e888"
   },
   "source": [
    "# Create a pytree mask of the trainable params.\n",
    "def is_trainable_param(name, param):  # pylint: disable=unused-argument\n",
    "    if name.startswith(\"llm/layers/attn/\"):  return True\n",
    "    if name.startswith(\"llm/\"):              return False\n",
    "    if name.startswith(\"img/\"):              return False\n",
    "    raise ValueError(f\"Unexpected param name {name}\")\n",
    "\n",
    "\n",
    "trainable_mask = tree_map_with_names(is_trainable_param, params)\n",
    "\n",
    "# If more than one device is available (e.g. multiple GPUs) the parameters can\n",
    "# be sharded across them to reduce HBM usage per device.\n",
    "mesh = jax.sharding.Mesh(jax.devices(), (\"data\"))\n",
    "\n",
    "data_sharding = jax.sharding.NamedSharding(\n",
    "    mesh, jax.sharding.PartitionSpec(\"data\"))\n",
    "\n",
    "params_sharding = infer_sharding(\n",
    "    params, strategy=[('.*', 'fsdp(axis=\"data\")')], mesh=mesh)\n",
    "\n",
    "# Yes: Some donated buffers are not usable.\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"Some donated buffers were not usable\")\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, donate_argnums=(0,), static_argnums=(1,))\n",
    "def maybe_cast_to_f32(params, trainable):\n",
    "    # Cast others to float16, since some GPUs don't support bf16.\n",
    "    return jax.tree.map(lambda p, m: p.astype(jnp.float32)\n",
    "    if m else p.astype(jnp.float16),\n",
    "                        params, trainable)\n",
    "\n",
    "\n",
    "# Loading all params in simultaneous - albeit much faster and more succinct -\n",
    "# requires more RAM than the T4 colab runtimes have by default.\n",
    "# Instead, do it param by param.\n",
    "params, treedef = jax.tree.flatten(params)\n",
    "sharding_leaves = jax.tree.leaves(params_sharding)\n",
    "trainable_leaves = jax.tree.leaves(trainable_mask)\n",
    "for idx, (sharding, trainable) in enumerate(zip(sharding_leaves, trainable_leaves)):\n",
    "    params[idx] = reshard(params[idx], sharding)\n",
    "    params[idx] = maybe_cast_to_f32(params[idx], trainable)\n",
    "    params[idx].block_until_ready()\n",
    "params = jax.tree.unflatten(treedef, params)\n",
    "\n",
    "\n",
    "# Print params to show what the model is made of.\n",
    "def parameter_overview(params):\n",
    "    for path, arr in tree_flatten_with_names(params)[0]:\n",
    "        print(f\"{path:80s} {str(arr.shape):22s} {arr.dtype}\")\n",
    "\n",
    "\n",
    "print(\" == Model params == \")\n",
    "parameter_overview(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "data_path = \"data/RISCM/\"\n",
    "images_folder = \"resized/\"\n",
    "captions_path = data_path + \"captions.csv\"\n",
    "\n",
    "df_input = pl.read_csv(captions_path, separator=\",\")\n",
    "splits = df_input.select('split').unique()['split'].to_list()\n",
    "for split in splits:\n",
    "    filename = f\"{data_path}{split}_captions.jsonl\"\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"{filename} not found, starting to process...\")\n",
    "        df_split = df_input.filter(pl.col('split') == split)\n",
    "        with open(filename, 'w') as f:\n",
    "            for row in df_split.iter_rows(named=True):\n",
    "                for i in range(1, 6):\n",
    "                    json_object = {\n",
    "                        \"prefix\": \"\",\n",
    "                        \"image\": f\"{images_folder}{row['image']}\",\n",
    "                        \"suffix\": row[f\"caption_{i}\"],\n",
    "                    }\n",
    "                    f.write(json.dumps(json_object) + '\\n')\n",
    "    else:\n",
    "        print(f\"{filename} is already processed, skipping...\")"
   ],
   "metadata": {
    "id": "UHJMJ5KopMLk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745505081059,
     "user_tz": -180,
     "elapsed": 71,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8SRW0NuU4UcW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745505089424,
     "user_tz": -180,
     "elapsed": 5,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     }
    }
   },
   "source": [
    "def preprocess_image(image, size=224):\n",
    "    # Model has been trained to handle images of different aspects ratios\n",
    "    # resized to 224x224 in the range [-1, 1]. Bilinear and antialias resize\n",
    "    # options are helpful to improve quality in some tasks.\n",
    "\n",
    "    image = np.asarray(image)\n",
    "    if image.ndim == 2:  # Convert image without last channel into greyscale.\n",
    "        image = np.stack((image,) * 3, axis=-1)\n",
    "    image = image[..., :3]  # Remove alpha layer.\n",
    "    assert image.shape[-1] == 3\n",
    "\n",
    "    image = tf.constant(image)\n",
    "    image = tf.image.resize(image, (size, size), method='bilinear', antialias=True)\n",
    "    return image.numpy() / 127.5 - 1.0  # [0, 255]->[-1,1]\n",
    "\n",
    "\n",
    "def preprocess_tokens(prefix, suffix=None, seqlen=None):\n",
    "    # Model has been trained to handle tokenized text composed of a prefix with\n",
    "    # full attention and a suffix with causal attention.\n",
    "    separator = \"\\n\"\n",
    "    tokens = tokenizer.encode(prefix, add_bos=True) + tokenizer.encode(separator)\n",
    "    mask_ar = [0] * len(tokens)  # 0 to use full attention for prefix.\n",
    "    mask_loss = [0] * len(tokens)  # 0 to not use prefix tokens in the loss.\n",
    "\n",
    "    if suffix:\n",
    "        suffix = tokenizer.encode(suffix, add_eos=True)\n",
    "        tokens += suffix\n",
    "        mask_ar += [1] * len(suffix)  # 1 to use causal attention for suffix.\n",
    "        mask_loss += [1] * len(suffix)  # 1 to use suffix tokens in the loss.\n",
    "\n",
    "    mask_input = [1] * len(tokens)  # 1 if it's a token, 0 if padding.\n",
    "    if seqlen:\n",
    "        padding = [0] * max(0, seqlen - len(tokens))\n",
    "        tokens = tokens[:seqlen] + padding\n",
    "        mask_ar = mask_ar[:seqlen] + padding\n",
    "        mask_loss = mask_loss[:seqlen] + padding\n",
    "        mask_input = mask_input[:seqlen] + padding\n",
    "\n",
    "    return jax.tree.map(np.array, (tokens, mask_ar, mask_loss, mask_input))\n",
    "\n",
    "\n",
    "def postprocess_tokens(tokens):\n",
    "    tokens = tokens.tolist()  # np.array to list[int]\n",
    "    try:  # Remove tokens at and after EOS if any.\n",
    "        eos_pos = tokens.index(tokenizer.eos_id())\n",
    "        tokens = tokens[:eos_pos]\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return tokenizer.decode(tokens)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "whzWOojGOtzi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1745505122323,
     "user_tz": -180,
     "elapsed": 1585,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     }
    }
   },
   "source": [
    "SEQLEN = 128\n",
    "\n",
    "train_dataset = DataSource(\n",
    "    os.path.join(data_path, \"train_captions.jsonl\"),\n",
    "    fopen_keys={\"image\": data_path}\n",
    ")\n",
    "\n",
    "val_dataset = DataSource(\n",
    "    os.path.join(data_path, \"val_captions.jsonl\"),\n",
    "    fopen_keys={\"image\": data_path})\n",
    "\n",
    "\n",
    "def train_data_iterator():\n",
    "    \"\"\"Never ending iterator over training examples.\"\"\"\n",
    "    # Shuffle examples and repeat so one can train for many epochs.\n",
    "    dataset = train_dataset.get_tfdata().shuffle(1_000).repeat()\n",
    "    for example in dataset.as_numpy_iterator():\n",
    "        image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # TODO: #\n",
    "        # numSegments = 14 * 14\n",
    "        # segments = slic(image, n_segments=numSegments, sigma=5)\n",
    "        # image = mark_boundaries(image, segments)\n",
    "        #########\n",
    "\n",
    "        prefix = \"caption en\"  # Could also be a different prefix per example.\n",
    "        suffix = example[\"suffix\"].decode().lower()\n",
    "        tokens, mask_ar, mask_loss, _ = preprocess_tokens(prefix, suffix, SEQLEN)\n",
    "\n",
    "        yield {\n",
    "            \"image\": np.asarray(image),\n",
    "            \"text\": np.asarray(tokens),\n",
    "            \"mask_ar\": np.asarray(mask_ar),\n",
    "            \"mask_loss\": np.asarray(mask_loss),\n",
    "        }\n",
    "\n",
    "\n",
    "def validation_data_iterator():\n",
    "    \"\"\"Single iterator over validation examples.\"\"\"\n",
    "    for example in val_dataset.get_tfdata(ordered=True).as_numpy_iterator():\n",
    "        image = Image.open(io.BytesIO(example[\"image\"]))\n",
    "        image = preprocess_image(image)\n",
    "\n",
    "        # TODO: #\n",
    "        # numSegments = 14 * 14\n",
    "        # segments = slic(image, n_segments=numSegments, sigma=5)\n",
    "        # image = mark_boundaries(image, segments)\n",
    "        #########\n",
    "\n",
    "        prefix = \"caption en\"  # Could also be a different prefix per example.\n",
    "        tokens, mask_ar, _, mask_input = preprocess_tokens(prefix, seqlen=SEQLEN)\n",
    "\n",
    "        yield {\n",
    "            \"image\": np.asarray(image),\n",
    "            \"text\": np.asarray(tokens),\n",
    "            \"mask_ar\": np.asarray(mask_ar),\n",
    "            \"mask_input\": np.asarray(mask_input),\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 1554,
     "status": "error",
     "timestamp": 1745505151122,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "BzJfb5t0nsLq",
    "outputId": "27002e27-3998-4b09-ae27-e32c180470c1"
   },
   "source": [
    "def render_inline(image, resize=(128, 128)):\n",
    "    \"\"\"Convert image into inline html.\"\"\"\n",
    "    image = Image.fromarray(image)\n",
    "    image.resize(resize)\n",
    "    with io.BytesIO() as buffer:\n",
    "        image.save(buffer, format='jpeg')\n",
    "        image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n",
    "        return f\"data:image/jpeg;base64,{image_b64}\"\n",
    "\n",
    "\n",
    "def render_example(image, caption):\n",
    "    image = ((image + 1) / 2 * 255).astype(np.uint8)  # [-1,1] -> [0, 255]\n",
    "    return f\"\"\"\n",
    "    <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n",
    "        <img style=\"width:128px; height:128px;\" src=\"{render_inline(image, resize=(64, 64))}\" />\n",
    "        <p style=\"width:256px; margin:10px; font-size:small;\">{html.escape(caption)}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "html_out = \"\"\n",
    "for idx, example in zip(range(8), train_data_iterator()):\n",
    "    caption = postprocess_tokens(example[\"text\"])  # detokenize model input.\n",
    "    caption = caption[len(\"caption en\\n\"):]  # strip prefix\n",
    "    html_out += render_example(example[\"image\"], caption)\n",
    "\n",
    "print(\"Training examples\")\n",
    "display(HTML(html_out))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dwUV_imW3WQJ"
   },
   "source": [
    "# The main update_fn using a simple stochastic gradient descent (SGD).\n",
    "@functools.partial(jax.jit, donate_argnums=(0,))\n",
    "def update_fn(params, batch, learning_rate):\n",
    "    imgs, txts, mask_ar = batch[\"image\"], batch[\"text\"], batch[\"mask_ar\"]\n",
    "\n",
    "    def loss_fn(params):\n",
    "        text_logits, _ = model.apply({\"params\": params}, imgs, txts[:, :-1], mask_ar[:, :-1], train=True)\n",
    "        logp = jax.nn.log_softmax(text_logits, axis=-1)\n",
    "\n",
    "        # The model takes as input txts[:, :-1] but the loss is defined as predicting\n",
    "        # next tokens txts[:, 1:]. Additionally, mask_loss[:, 1:] indicates which tokens\n",
    "        # are part of the loss (e.g. prefix and padded tokens are not included).\n",
    "        mask_loss = batch[\"mask_loss\"][:, 1:]\n",
    "        targets = jax.nn.one_hot(txts[:, 1:], text_logits.shape[-1])\n",
    "\n",
    "        # Compute the loss per example. i.e. the mean of per token pplx.\n",
    "        # Since each example has a different number of tokens, normalize it.\n",
    "        token_pplx = jnp.sum(logp * targets, axis=-1)  # sum across vocab_size.\n",
    "        example_loss = -jnp.sum(token_pplx * mask_loss, axis=-1)  # sum across seq_len.\n",
    "        example_loss /= jnp.clip(jnp.sum(mask_loss, -1), 1)  # weight by num of tokens.\n",
    "\n",
    "        # batch_loss: mean of per example loss.\n",
    "        return jnp.mean(example_loss)\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "\n",
    "    # Apply gradients to trainable params using SGD.\n",
    "    def apply_grad(param, gradient, trainable):\n",
    "        if not trainable: return param\n",
    "        return param - learning_rate * gradient\n",
    "\n",
    "    params = jax.tree_util.tree_map(apply_grad, params, grads, trainable_mask)\n",
    "\n",
    "    return params, loss\n",
    "\n",
    "\n",
    "# Evaluation/inference loop.\n",
    "def make_predictions(data_iterator, *, num_examples=None,\n",
    "                     batch_size=4, seqlen=SEQLEN, sampler=\"greedy\"):\n",
    "    outputs = []\n",
    "    while True:\n",
    "        # Construct a list of examples in the batch.\n",
    "        examples = []\n",
    "        try:\n",
    "            for _ in range(batch_size):\n",
    "                examples.append(next(data_iterator))\n",
    "                examples[-1][\"_mask\"] = np.array(True)  # Indicates true example.\n",
    "        except StopIteration:\n",
    "            if len(examples) == 0:\n",
    "                return outputs\n",
    "\n",
    "        # Not enough examples to complete a batch. Pad by repeating last example.\n",
    "        while len(examples) % batch_size:\n",
    "            examples.append(dict(examples[-1]))\n",
    "            examples[-1][\"_mask\"] = np.array(False)  # Indicates padding example.\n",
    "\n",
    "        # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "        batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "        batch = reshard(batch, data_sharding)\n",
    "\n",
    "        # Make model predictions\n",
    "        tokens = decode({\"params\": params}, batch=batch,\n",
    "                        max_decode_len=seqlen, sampler=sampler)\n",
    "\n",
    "        # Fetch model predictions to device and detokenize.\n",
    "        tokens, mask = jax.device_get((tokens, batch[\"_mask\"]))\n",
    "        tokens = tokens[mask]  # remove padding examples.\n",
    "        responses = [postprocess_tokens(t) for t in tokens]\n",
    "\n",
    "        # Append to html output.\n",
    "        for example, response in zip(examples, responses):\n",
    "            outputs.append((example[\"image\"], response))\n",
    "            if num_examples and len(outputs) >= num_examples:\n",
    "                return outputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1040715,
     "status": "ok",
     "timestamp": 1744926126923,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "067wj_6bZAG3",
    "outputId": "ae0de3bb-d7d2-484a-9b18-fbef69842d67"
   },
   "source": [
    "# Run a short training loop with cosine learning rate schedule.\n",
    "#\n",
    "# Note: the first step can be quite slow on some machines (up to several minutes)\n",
    "# due to XLA compilation of the jax.jit'd function.\n",
    "#\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "TRAIN_EXAMPLES = 512\n",
    "LEARNING_RATE = 0.03\n",
    "\n",
    "TRAIN_STEPS = TRAIN_EXAMPLES // BATCH_SIZE\n",
    "EVAL_STEPS = TRAIN_STEPS // 4\n",
    "\n",
    "train_data_it = train_data_iterator()\n",
    "\n",
    "sched_fn = create_learning_rate_schedule(\n",
    "    total_steps=TRAIN_STEPS + 1, base=LEARNING_RATE,\n",
    "    decay_type=\"cosine\", warmup_percent=0.10)\n",
    "\n",
    "for step in range(1, TRAIN_STEPS + 1):\n",
    "    # Make list of N training examples.\n",
    "    examples = [next(train_data_it) for _ in range(BATCH_SIZE)]\n",
    "\n",
    "    # Convert list of examples into a dict of np.arrays and load onto devices.\n",
    "    batch = jax.tree.map(lambda *x: np.stack(x), *examples)\n",
    "    batch = reshard(batch, data_sharding)\n",
    "\n",
    "    # Training step and report training loss\n",
    "    learning_rate = sched_fn(step)\n",
    "    params, loss = update_fn(params, batch, learning_rate)\n",
    "\n",
    "    loss = jax.device_get(loss)\n",
    "    print(f\"step: {step:2d}/{TRAIN_STEPS:2d}   lr: {learning_rate:.5f}   loss: {loss:.4f}\")\n",
    "\n",
    "    if (step % EVAL_STEPS) == 0:\n",
    "        print(f\"Model predictions at step {step}\")\n",
    "        html_out = \"\"\n",
    "        for image, caption in make_predictions(\n",
    "                validation_data_iterator(), num_examples=4, batch_size=4):\n",
    "            html_out += render_example(image, caption)\n",
    "        display(HTML(html_out))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 45725,
     "status": "ok",
     "timestamp": 1744926172646,
     "user": {
      "displayName": "Eren Narin",
      "userId": "16039153764301371561"
     },
     "user_tz": -180
    },
    "id": "hgUhEKjzPdMQ",
    "outputId": "ccaeb8ac-308d-4e33-f4f1-c9bf8eca9f0c"
   },
   "source": [
    "# The validation data consists of 10 images in a different domain than training\n",
    "# data.\n",
    "%%time\n",
    "\n",
    "print(\"Model predictions\")\n",
    "html_out = \"\"\n",
    "for image, caption in make_predictions(validation_data_iterator(), batch_size=4):\n",
    "    html_out += render_example(image, caption)\n",
    "display(HTML(html_out))\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/paligemma/fine-tuning-paligemma.ipynb",
     "timestamp": 1744917295319
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
